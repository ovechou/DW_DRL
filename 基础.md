# 强化学习



[toc]

## 说明

- Note为一些问题和主要知识点的汇总

- 思维导图

![Clipboard - 2020-10-21 12.54.34](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/Clipboard%20-%202020-10-21%2012.54.34.png)



## 基础概念

强化学习概念图1，2

![Clipboard - 2020-10-21 12.56.44](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/Clipboard%20-%202020-10-21%2012.57.36.png)

![图片2](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/Clipboard%20-%202020-10-21%2012.56.44.png)


​	在每一步t，智能体：获得观察O_t,获得奖励R_t,执行行动A_t，环境：获得行动A_t,给出观察O_{t+1},给出奖励R_{t+1}

一个 智能体(agent) 怎么在一个复杂不确定的环境(environment)里面去极大化它能获得的奖励.

直观简单的快速了解可以看[莫烦python](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL/)的视频，了解一个强化学习的概念。（不涉及数学公式推导，作为辅助理解。

### 主要元素

1. environment
2. Agent

### 主要流程

Agent 在环境里面获取到状态，agent 会利用这个状态输出一个 action，一个决策。然后这个决策会放到环境之中去，环境会通过这个 agent 采取的决策，输出下一个状态以及当前的这个决策得到的奖励。

### 目标

最大化Reward

### Note

- 由于不断地交互，数据具有强相关性。为序列数据。

- [ ] 学习到什么？智能体的输出是什么？

- [ ] 奖励是什么？需要怎么获取？



## Sequential Decision Making

Sequential Decision Making（序列决策过程）

### 奖励的描述

- [x] **奖励是环境给的一个反馈信号**，这个信号指定了这个 agent 在某一步采取了某个策略是否得到奖励。**奖励可正可负**。分为正向奖励和负向奖励。

- [ ] 奖励分为即时奖励和长远奖励。需要在这两者之间取得平衡。如何在这两者之间取得平衡，在后续马尔可夫过程中详细展开。强化学习里面一个重要的课题就是近期奖励和远期奖励的一个 trade-off。

### 序列信息是什么？

在跟环境的交互过程中，agent 会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。**所以历史是观测、行为、奖励的序列：**

$$\mathrm{H}_{\mathrm{t}}=\mathrm{O}_{1}, \mathrm{R}_{1}, \mathrm{A}_{1}, \ldots, \mathrm{A}_{\mathrm{t}-1}, \mathrm{O}_{\mathrm{t}}, \mathrm{R}_{\mathrm{t}}$$

Agent 在采取当前动作的时候会依赖于它之前得到的这个历史，**所以你可以把整个游戏的状态看成关于这个历史的函数：**

$\mathrm{S}_{\mathrm{t}}=\mathrm{f}\left(\mathrm{H}_{\mathrm{t}}\right)$

- 观测和状态的关系

  ==观测==是对状态的部分描述，可能会有部分信息的缺失。==状态==是对世界的完整描述。

  举个🌰：一张报纸下压着一百块钱，观测=一张报纸，状态=一张报纸+一百块钱。

在 agent 的内部也有一个函数来更新这个状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是 `full observability`，就是全部可以观测。换句话说，当 agent 能够观察到环境的所有状态时，我们称这个环境是`完全可观测的(fully observed)`。

当 agent 只能看到部分的观测，我们就称这个环境是`部分可观测的(partially observed)`。在这种情况下面，强化学习通常被建模成一个 POMDP 的问题。

大部分情况下我们遇到的都是部分可观测的环境，即POMDP的问题。

POMDP 可以用一个 7 元组描述：$(\mathrm{S}, \mathrm{A}, \mathrm{T}, \mathrm{R}, \Omega, \mathrm{O}, \gamma)$，其中 S 表示状态空间，为隐变量，A 为动作空间，$\mathrm{T}\left(\mathrm{s}^{\prime} \mid \mathrm{s}, \mathrm{a}\right)$ 为状态转移概率，R 为奖励函数，$\Omega(o|s,a)$ 为观测概率，O 为观测空间，$\gamma $为折扣系数。

### Note

- [ ] 主要的各项参数分别代表什么？需要进一步说明

## Action Space

不同的环境允许不同种类的动作。在给定的环境中，有效动作的集合经常被称为`动作空间(action space)`。像 Atari 和 Go 这样的环境有`离散动作空间(discrete action spaces)`，在这个动作空间里，agent 的动作数量是有限的。在其他环境，比如在物理世界中控制一个 agent，在这个环境中就有`连续动作空间(continuous action spaces)` 。在连续空间中，动作是实值的向量。

举个🌰

- 走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间；
- 如果机器人向$ 360^{\circ}$ 中的任意角度都可以移动，则为连续动作空间。

## Major Components of RL Agent

- 首先 agent 有一个 `policy function`，agent 会用这个函数来选取下一步的动作。

- 然后它也可能生成一个`价值函数(value function)`。我们用价值函数来对当前状态进行估价，它就是说你进入现在这个状态，可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。

  ==policy function==和==value function==之间可以相互转换。

- 另外一个组成成分是`模型(model)`。模型表示了 agent 对这个环境的状态进行了理解，它决定了这个世界是如何进行的。

### Policy

Policy 决定了这个 agent 的行为，它其实是一个函数，把输入的状态变成行为。这里有两种 policy：

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.26.png)

- 一种是 `stochastic policy(随机性策略)`，它就是 $\pi$ 函数 $\pi(a | s)=P\left[A_{t}=a | S_{t}=s\right]$ 。当你输入一个状态 s的时候，输出是一个概率。这个概率就是你所有行为的一个概率，然后你可以进一步对这个概率分布进行采样，得到真实的你采取的行为。比如说这个概率可能是有 70% 的概率往左，30% 的概率往右，那么你通过采样就可以得到一个 action。

  $\pi(a \mid s)=P\left(A_{t}=a \mid S_{t}=s\right)$


- 一种是 `deterministic policy(确定性策略)`，就是说你这里有可能只是采取它的极大化，采取最有可能的动作。你现在这个概率就是事先决定好的。

  $a=\pi(s)$

  从 Atari 游戏来看的话，policy function 的输入就是游戏的一帧，它的输出决定你是往左走或者是往右走。

  通常情况下，强化学习一般使用`随机性策略`。随机性策略有很多优点：

- 在学习时可以通过引入一定随机性来更好地探索环境；

- 随机性策略的动作具有多样性，这一点在多个智能体博弈时也非常重要。采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测。

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.31.png)

### Value Function

**价值函数是一个折扣的未来奖励的加和**，就是你通过进行某一种行为，然后你未来得到多大的奖励。

价值函数是对于未来累积奖励的预测，用于评估给定策略下，状态的好坏.价值函数是一个`标量`，长期而言什么是好的

$v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots \mid S_{t}=s\right]$

价值函数里面有一个 discount factor。我们希望尽可能在短的时间里面得到尽可能多的奖励。如果我们说十天过后，我给你 100 块钱，跟我现在给你 100 块钱，你肯定更希望我现在就给你 100 块钱，因为你可以把这 100 块钱存在银行里面，你就会有一些利息。所以我们就通过把这个 `discount factor` 放到价值函数的定义里面，后面得到的奖励价值函数的定义其实是一个期望。

对于这个奖励函数，我们另外还有一个 Q 函数。Q 函数里面包含两个变量：状态和动作。所以你未来可以获得多少的奖励，它的这个期望取决于你当前的状态和当前的行为。这个 Q 函数是强化学习算法在学习的一个函数。因为当我们得到这个 Q 函数的过后，进入某一种状态，它最优的行为其实就可以通过这个 Q 函数来得到。

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.32.png)

#### Note

==discount factor==此处用来平衡长远奖励和当前奖励。

### Model

用于模拟环境的行为，预测下一个状态

**模型决定了下一个状态会是什么样的，就是说下一步的状态取决于你当前的状态以及你当前采取的行为。**它由两个部分组成，一个是 probability，它这个转移状态之间是怎么转移的。另外是这个奖励函数，当你在当前状态采取了某一个行为，可以得到多大的奖励。

$\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right]$

$\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]$

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.29.png)

#### Note

- [ ] 马尔可夫过程详细解读 

## 强化学习分类

### 1. 根据是否建立环境模型分类



**1.1 基于模型的强化学习（model-based）**

模型可以被环境所知道，agent可以直接利用模型执行下一步的动作，而无需与实际环境进行交互学习。

比如：围棋、迷宫

具体来说，当智能体知道状态转移函数 $P(s_{t+1}|s_t,a_t)$和奖励函数 $R(s_t,a_t)$后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为`有模型学习`。

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.35.png)

**1.2 模型无关的强化学习（model-free）**

它没有去直接估计这个状态的转移，也没有得到环境的具体转移变量。它通过学习 value function 和 policy function 进行决策。这种 model-free 的模型里面没有一个环境转移的一个模型。

真正意义上的强化学习，环境是黑箱。现实中的情况大多是这种

没有环境模型

比如Atari游戏，需要大量的采样

![img](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8aee6642-1717-43f8-ac1e-acdcf236a5b3/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20201019%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20201019T070532Z&X-Amz-Expires=86400&X-Amz-Signature=f1c2cb152a4da5b0d4ba89ef2423b96c98d8b8f5f913fca4f03c53d6e677b2f0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

#### Note

- [ ] 有模型强化学习和免模型强化学习有什么区别？

  **核心点在于是否需要对真实环境建模.**

目前，大部分深度强化学习方法都采用了免模型学习，这是因为：

- 免模型学习更为简单直观且有丰富的开源资料，像 DQN、AlphaGo 系列等都采用免模型学习；
- 在目前的强化学习研究中，大部分情况下环境都是静态的、可描述的，智能体的状态是离散的、可观察的（如 Atari 游戏平台），这种相对简单确定的问题并不需要评估状态转移函数和奖励函数，直接采用免模型学习，使用大量的样本进行训练就能获得较好的效果。

### 2. 根据如何获得策略分类

1. **基于价值的强化学习**

   ==没有策略（隐含）、价值函数==

   显式地学习的是价值函数，隐式地学习了它的策略。因为这个策略是从我们学到的价值函数里面推算出来的。

   举个🌰：

   ​	走迷宫游戏，向左向右的**value**分别为👈：20，👉：10

​		    通过比较两者之间的价值，得到👈的动作更有利，于是👈。

2. **基于策略的强化学习**

   ==策略、没有价值函数==

   它直接去学习 policy，就是说你直接给它一个 state，它就会输出这个动作的概率。在这个 policy-based agent 里面并没有去学习它的价值函数。

   举个🌰：

   ​	走迷宫游戏，根据当前的位置，得到向左向右的**概率**分别为👈：67%，👉：33%。

   ​	 根据向左向右的概率，选择动作。

3. **Actor-Critic**

   ==策略、价值函数==

   把 value-based 和 policy-based 结合起来就有了 `Actor-Critic agent`。这一类 agent 就把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个最佳的行为。

#### Note

基于策略迭代和基于价值迭代的强化学习方法有什么区别?

在`基于策略迭代`的强化学习方法中，智能体会`制定一套动作策略`（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。

而在`基于价值迭代`的强化学习方法中，智能体不需要制定显式的策略，它`维护一个价值表格或价值函数`，并通过这个价值表格或价值函数来选取价值最大的动作。**基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），**对于行为集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)。

基于价值迭代的强化学习算法有 Q-learning、 Sarsa 等，而基于策略迭代的强化学习算法有策略梯度算法等。此外， Actor-Critic 算法同时使用策略和价值评估来做出决策，其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。

强化学习分类图

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.36.png)



## Exploration and Exploitation

在强化学习里面，`Exploration` 和` Exploitation` 是两个很核心的问题。

- Exploration 是说我们怎么去探索这个环境，通过尝试不同的行为来得到一个最佳的策略，得到最大奖励的策略。
- Exploitation 是说我们不去尝试新的东西，就采取已知的可以得到很大奖励的行为。

因为在刚开始的时候强化学习 agent 不知道它采取了某个行为会发生什么，所以它只能通过试错去探索。所以 Exploration 就是在试错来理解采取的这个行为到底可不可以得到好的奖励。Exploitation 是说我们直接采取已知的可以得到很好奖励的行为。所以这里就面临一个==trade-off==，怎么通过牺牲一些短期的 reward 来获得行为的理解。

举个🌰

​	在一条街吃饭，有20家餐厅，需要怎么去尽可能地找到我们喜欢的那家餐厅。在不同的情况下==trade–off==不一样。

	- 前9家都很难吃，第10家非常好吃。
	- 前3家都很好吃，后面5家很难吃。

## 实践

比较推荐OpenAI和Gym。

在安装的过程中，推荐使用Anaconda+VSCode的方案。相对来说比较轻便。

[Anaconda使用指南](https://www.jianshu.com/p/eaee1fadc1e9)

[Anaconda介绍](https://zhuanlan.zhihu.com/p/32925500)



## 习题

- 强化学习的基本结构是什么？

  Agent和Environment

  当Agent在Environment中得到当前时刻的State，Agent会基于此状态输出一个Action。然后这个Action会加入到Environment中去并输出下一个State和当前的这个Action得到的Reward。Agent在Environment里面存在的目的就是为了极大它的期望积累的Reward。

![图片3](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/Clipboard%20-%202020-10-21%2012.56.44.png)

- 强化学习相对于监督学习为什么训练会更加困难？（强化学习的特征）

  1. 强化学习处理的多是序列数据，其很难像监督学习的样本一样满足**IID（独立同分布）**条件。

     数据具有较强的连续性，在实际操作中，需要使用随机打乱的方法。

  2. 强化学习有奖励的延迟（Delay Reward），即在Agent的action作用在Environment中时，Environment对于Agent的State的**奖励的延迟**（Delayed Reward），使得反馈不及时。

     奖励和动作并不像监督学习标签和数据之间一一对应，具有强关联性；同时奖励需要有一个长远奖励和短期奖励的平衡，举个🌰，在一个迷宫里，往前走一步是一个小奶酪，但是到了吃了奶酪之后会掉下悬崖，导致一个很坏的结果；长远奖励和短期奖励之间的平衡。

  3. 相比于监督学习有正确的label，可以通过其修正自己的预测，强化学习相当于一个“试错”的过程，其完全根据Environment的“**反馈**”更新对自己最有利的Action。

  1. 有**trial-and-error exploration**的过程，即需要通过探索Environment来获取对这个Environment的理解。

  2. 强化学习的Agent会从Environment里面获得**延迟**的Reward。

  3. 强化学习的训练过程中**时间**非常重要，因为数据都是有时间关联的，而不是像监督学习一样是IID分布的。

  4. 强化学习中Agent的Action会**影响**它随后得到的**反馈**。

     ![RL3.png](https://mofanpy.com/static/results-small/ML-intro/RL3.png)

我们知道监督学习, 是已经有了数据和数据对应的正确标签, 比如这样. 监督学习就能学习出那些脸对应哪种标签. 不过强化学习还要更进一步, 一开始它并没有数据和标签.

他要通过一次次在环境中的尝试, 获取这些数据和标签, 然后再学习通过哪些数据能够对应哪些标签, 通过学习到的这些规律, 竟可能地选择带来高分的行为 (比如这里的开心脸). 这也就证明了在强化学习中, 分数标签就是他的老师, 他和监督学习中的老师也差不多.

- 近几年强化学习发展迅速的原因？
  1. **算力（GPU、TPU）的提升**，我们可以更快地做更多的 trial-and-error 的尝试来使得Agent在Environment里面获得很多信息，取得更大的Reward。
  2. 我们有了深度强化学习这样一个端到端的训练方法，可以把特征提取和价值估计或者决策一起优化，这样就可以得到一个更强的决策网络。

- 状态和观测有什么关系？

  状态（state）是对世界的**完整描述**，不会隐藏世界的信息。观测（observation）是对状态的**部分描述**，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用一个实值向量、矩阵或者更高阶的张量来表示状态和观测。

- 对于一个强化学习 Agent，它由什么组成？

1. **策略函数（policy function）**，Agent会用这个函数来选取它下一步的动作，包括**随机性策略（stochastic policy）**和**确定性策略（deterministic policy）**。
2. **价值函数（value function）**，我们用价值函数来对当前状态进行评估，即进入现在的状态，到底可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。
3. **模型（model）**，其表示了 Agent 对这个Environment的状态进行的理解，它决定了这个系统是如何进行的。

- 根据强化学习 Agent 的不同，我们可以将其分为哪几类？

1. **基于价值函数的Agent**。 显式学习的就是价值函数，隐式的学习了它的策略。因为这个策略是从我们学到的价值函数里面推算出来的。
2. **基于策略的Agent**。它直接去学习 policy，就是说你直接给它一个 state，它就会输出这个动作的概率。然后在这个 policy-based agent 里面并没有去学习它的价值函数。
3. 然后另外还有一种 Agent 是把这两者结合。把 value-based 和 policy-based 结合起来就有了 **Actor-Critic agent**。这一类 Agent 就把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个更佳的状态。

- 基于策略迭代和基于价值迭代的强化学习方法有什么区别?

1. 基于策略迭代的强化学习方法，agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。
2. 基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于行为集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)；
3. 基于价值迭代的强化学习算法有 Q-learning、 Sarsa 等，而基于策略迭代的强化学习算法有策略梯度算法等。
4. 此外， Actor-Critic 算法同时使用策略和价值评估来做出决策，其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。

- 有模型（model-based）学习和免模型（model-free）学习有什么区别？

  ​	针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。 有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。总的来说，有模型学习相比于免模型学习仅仅多出一个步骤，即对真实环境进行建模。免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。免模型学习的泛化性要优于有模型学习，原因是有模型学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。

- 强化学习的通俗理解

  environment 跟 reward function 不是我们可以控制的，environment 跟 reward function 是在开始学习之前，就已经事先给定的。我们唯一能做的事情是调整 actor 里面的 policy，使得 actor 可以得到最大的 reward。Actor 里面会有一个 policy， 这个 policy 决定了actor 的行为。Policy 就是给一个外界的输入，然后它会输出 actor 现在应该要执行的行为。

  [莫烦python RL介绍视频](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL/),不涉及数学公式，可以对通俗理解做一个进一步的解释。

  