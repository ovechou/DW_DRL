# 策略梯度

[toc]





# 思维导图

![思维导图——策略梯度](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/Clipboard%20-%202020-10-29%2011.01.58.png)

# 基础概念

Policy 一般写成 $\pi$。假设你是用 deep learning 的技术来做 reinforcement learning 的话，**policy 就是一个 network**。Network 里面就有一堆参数， 我们用 $\theta$ 来代表 $\pi$ 的参数。

- Input：输入的状态。（在交通中就是交通流信息，比如车辆排队长度、车辆等待时长等

- Output：动作

- episode：一个回合，需要有结束状态或者有终止的回合数。
- total reward：在某个episode中的所有reward的总合。

- Trajectory：某场游戏里environment输出的s和actor输出的a的序列。

$$
\begin{aligned}
\mathrm{p}_{\theta}(\tau) &=\mathrm{p}\left(\mathrm{s}_{1}\right) \mathrm{p}_{\theta}\left(\mathrm{a}_{1} \mid \mathrm{s}_{1}\right) \mathrm{p}\left(\mathrm{s}_{2} \mid \mathrm{s}_{1}, \mathrm{a}_{1}\right) \mathrm{p}_{\theta}\left(\mathrm{a}_{2} \mid \mathrm{s}_{2}\right) \mathrm{p}\left(\mathrm{s}_{3} \mid \mathrm{s}_{2}, \mathrm{a}_{2}\right) \cdots \\
&=\mathrm{p}\left(\mathrm{s}_{1}\right) \prod_{t=1}^{\mathrm{T}} \mathrm{p} \theta\left(\mathrm{a}_{t} \mid \mathrm{s}_{\mathrm{t}}\right) \mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)
\end{aligned}
$$

- reward function

  Reward function 根据在某一个 state 采取的某一个 action 决定说现在这个行为可以得到多少的分数。 在某一场游戏里面， 某一个 episode 里面，我们会得到 R。**我们要做的事情就是调整 actor 内部的参数 $\theta$， 使得 R 的值越大越好。** 

  $\overline{\mathbf{R}}_{\theta}=\sum_{\tau} \mathbf{R}(\tau) \mathbf{p}_{\theta}(\tau)=\mathbf{E}_{\tau \sim p \theta(\tau)}[\mathbf{R}(\tau)]$

## 概率

1. environment的行动。即Actor输入动作a后环境的反应，提前定义好，无法修改.

2. ==agent的行为。==我们可以控制的$\mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)$， 环境把$s_{t}$给到agent后，actor的反应，着就是我们需要去修改的。

  ![img](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/4.6.png)

## 基础流程

![img](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/4.8.png)

一般 policy gradient sample 的 data 就只会用一次。你把这些 data sample 起来，然后拿去 update 参数，这些 data 就丢掉了。接着再重新 sample data，才能够去 update 参数， 等一下我们会解决这个问题。（在实际操作的过程中，会有一个Memory，从memory中提取数据，不执行放回操作）

## Tips1： Add a Baseline

A2C方法就是使用这种方法来进行校准。



# 时序差分强化学习VS蒙特卡洛强化学习

![时序差分VS蒙特卡洛](https://picovechou.oss-cn-shenzhen.aliyuncs.com/img/Clipboard%20-%202020-10-29%2011.34.05.png)

# PPO算法

## 先验知识

- 如果要 learn 的 agent 跟和环境互动的 agent 是同一个的话， 这个叫做`on-policy(同策略)`。
- 如果要 learn 的 agent 跟和环境互动的 agent 不是同一个的话， 那这个叫做`off-policy(异策略)`。

## 核心点

两个Agent分工协作

因为现在跟环境做互动是 $\theta'$ 而不是 $\theta$。所以 sample 出来的东西跟 $\theta$ 本身是没有关系的。所以你就可以让$ \theta'$ 做互动 sample 一大堆的 data，$\theta$可以 update 参数很多次，一直到 $\theta$ train 到一定的程度，update 很多次以后，$\theta'$再重新去做 sample，这就是 on-policy 换成 off-policy 的妙用。

我们可以把 on-policy 换成 off-policy，但 importance sampling 有一个 issue，如果 $p_{\theta}\left(a_{t} | s_{t}\right)$跟$ p_{\theta'}\left(a_{t} | s_{t}\right)$差太多的话，这两个 distribution 差太多的话，importance sampling 的结果就会不好。怎么避免它差太多呢？这个就是 `Proximal Policy Optimization (PPO) ` 在做的事情。

**所以我们真正在意的是这个 actor 的行为上的差距，而不是它们参数上的差距。**

(也就是你要参考一个跟你文化水平差不多的人，而不是一个物理位置跟你差不多的人，尽可能 思维方式接近。)